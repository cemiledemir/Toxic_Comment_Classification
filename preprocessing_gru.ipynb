{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDALbP0O282q"
      },
      "source": [
        "#Bidirectional Gated Recurrent Unit\n",
        "##Word Embedding: GloVe 840B 300d\n",
        "##Optimizers: Adam\n",
        "**Word Embeddings**\n",
        "\n",
        "Words and ngrams have been primarily used as features in text classification. I did vectorized the texts using bag of words based features, now I will rely on neural embedding models. I will use a pre-trained embedding model. There are several pre-trained word2vec models trained on large corpora that one can download from the internet. GloVe(Global Vector) is one of popular pre-trained embedding algorithm from Stanford.\n",
        "\n",
        "The advantage of using embedding based features is that they create a dense, low-dimensional feature representation instead of the sparse, high-dimensional structure of bag of words/TFIDF and other such features.\n",
        "\n",
        "**GloVe Embeddings**\n",
        "\n",
        "GloVe is commonly used method of obtaining pre-trained embeddings. GloVe aims to achieve two goals:\n",
        "\n",
        "Create word vectors that capture meaning in vector space Takes advantage of global count statistics instead of only local information There are a lot of online material available to explain the concept about GloVe. So my focus here will be on, how to use pre-trained Glove word embeddings. I will provide relevant resources to look into more details.\n",
        "\n",
        "**Download the pre-trained glove file:** I will be using glove.6B file which is trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5NEW8BN2wI1",
        "outputId": "bdf47808-1105-4b50-be1f-96d4c858df4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 14:54:05--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2020-10-09 14:54:05--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2020-10-09 14:54:05--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  1.97MB/s    in 16m 56s \n",
            "\n",
            "2020-10-09 15:11:02 (2.04 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ip9ZTQ2xiT"
      },
      "source": [
        "!unzip -q ./glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Wj4gY-Om0AVg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import regularizers, initializers, optimizers, callbacks\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, Callback\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JOxjDWRK0AVk"
      },
      "source": [
        "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
        "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\n",
        "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
        "EMBEDDING_DIM = 300      # embedding dimensions for word vectors (word2vec/GloVe)\n",
        "GLOVE_DIR = \"../input/glove840b300dtxt/glove.840B.300d.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "nopSrLdy0AVn"
      },
      "source": [
        "train = pd.read_csv('../input/toxic-classification-trainset/train.csv')\n",
        "test = pd.read_csv('../input/toxic-classification-testset/test.csv')\n",
        "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARoxx6XL3RC3"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j-Dd9O7n0AVr"
      },
      "source": [
        "appo = {\n",
        "\"aren't\" : \"are not\",\n",
        "\"can't\" : \"can not\",\n",
        "\"couldn't\" : \"could not\",\n",
        "\"didn't\" : \"did not\",\n",
        "\"don't\" : \"do not\",\n",
        "\"hadn't\" : \"had not\",\n",
        "\"hasn't\" : \"has not\",\n",
        "\"haven't\" : \"have not\",\n",
        "\"he'd\" : \"he would\",\n",
        "\"he'll\" : \"he will\",\n",
        "\"he's\" : \"he is\",\n",
        "\"i'd\" : \"i would\",\n",
        "\"i'd\" : \"i had\",\n",
        "\"i'll\" : \"i will\",\n",
        "\"i'm\" : \"I am\",\n",
        "\"isn't\" : \"is not\",\n",
        "\"it's\" : \"it is\",\n",
        "\"it'll\":\"it will\",\n",
        "\"i've\" : \"I have\",\n",
        "\"let's\" : \"let us\",\n",
        "\"mightn't\" : \"might not\",\n",
        "\"mustn't\" : \"must not\",\n",
        "\"shan't\" : \"shall not\",\n",
        "\"she'd\" : \"she would\",\n",
        "\"she'll\" : \"she will\",\n",
        "\"she's\" : \"she is\",\n",
        "\"shouldn't\" : \"should not\",\n",
        "\"that's\" : \"that is\",\n",
        "\"there's\" : \"there is\",\n",
        "\"they'd\" : \"they would\",\n",
        "\"they'll\" : \"they will\",\n",
        "\"they're\" : \"they are\",\n",
        "\"they've\" : \"they have\",\n",
        "\"we'd\" : \"we would\",\n",
        "\"we're\" : \"we are\",\n",
        "\"weren't\" : \"were not\",\n",
        "\"we've\" : \"we have\",\n",
        "\"what'll\" : \"what will\",\n",
        "\"what're\" : \"what are\",\n",
        "\"what's\" : \"what is\",\n",
        "\"what've\" : \"what have\",\n",
        "\"where's\" : \"where is\",\n",
        "\"who'd\" : \"who would\",\n",
        "\"who'll\" : \"who will\",\n",
        "\"who're\" : \"who are\",\n",
        "\"who's\" : \"who is\",\n",
        "\"who've\" : \"who have\",\n",
        "\"won't\" : \"will not\",\n",
        "\"wouldn't\" : \"would not\",\n",
        "\"you'd\" : \"you would\",\n",
        "\"you'll\" : \"you will\",\n",
        "\"you're\" : \"you are\",\n",
        "\"you've\" : \"you have\",\n",
        "\"'re\": \" are\",\n",
        "\"wasn't\" : \"was not\",\n",
        "\"we'll\" : \" will\",\n",
        "\"tryin'\" : \"trying\",\n",
        "\"yay!\" : \" good \",\n",
        "\"yay\" : \" good \",\n",
        "\"yaay\" : \" good \",\n",
        "\"yaaay\" : \" good \",\n",
        "\"yaaaay\" : \" good \",\n",
        "\"yaaaaay\" : \" good \",\n",
        "\":/\" : \" bad \",\n",
        "\":&gt;\" : \" sad \",\n",
        "\":')\" : \" sad \",\n",
        "\":-(\" : \" frown \",\n",
        "\":(\" : \" frown \",\n",
        "\":s\": \" frown \",\n",
        "\":-s\": \" frown \",\n",
        "\"&lt;3\": \" heart \",\n",
        "\":d\": \" smile \",\n",
        "\":p\": \" smile \",\n",
        "\":dd\": \" smile \",\n",
        "\"8)\": \" smile \",\n",
        "\":-)\": \" smile \",\n",
        "\":)\": \" smile \",\n",
        "\";)\": \" smile \",\n",
        "\"(-:\": \" smile \",\n",
        "\"(:\": \" smile \",\n",
        "\":/\": \" worry \",\n",
        "\":&gt;\": \" angry \",\n",
        "\":')\": \" sad \",\n",
        "\":-(\": \" sad \",\n",
        "\":(\": \" sad \",\n",
        "\":s\": \" sad \",\n",
        "\":-s\": \" sad \",\n",
        "r\"\\br\\b\": \"are\",\n",
        "r\"\\bu\\b\": \"you\",\n",
        "r\"\\bhaha\\b\": \"ha\",\n",
        "r\"\\bhahaha\\b\": \"ha\",\n",
        "r\"\\bdon't\\b\": \"do not\",\n",
        "r\"\\bdoesn't\\b\": \"does not\",\n",
        "r\"\\bdidn't\\b\": \"did not\",\n",
        "r\"\\bhasn't\\b\": \"has not\",\n",
        "r\"\\bhaven't\\b\": \"have not\",\n",
        "r\"\\bhadn't\\b\": \"had not\",\n",
        "r\"\\bwon't\\b\": \"will not\",\n",
        "r\"\\bwouldn't\\b\": \"would not\",\n",
        "r\"\\bcan't\\b\": \"can not\",\n",
        "r\"\\bcannot\\b\": \"can not\",\n",
        "r\"\\bi'm\\b\": \"i am\",\n",
        "\"m\": \"am\",\n",
        "\"r\": \"are\",\n",
        "\"u\": \"you\",\n",
        "\"haha\": \"ha\",\n",
        "\"hahaha\": \"ha\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"cannot\": \"can not\",\n",
        "\"its\" : \"it is\",\n",
        "\"'s\" : \" is\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AFCd1F2y0AVu"
      },
      "source": [
        "keys = [i for i in appo.keys()]\n",
        "\n",
        "new_train_data = []\n",
        "ltr = train[\"comment_text\"].tolist()\n",
        "for i in ltr:\n",
        "    arr = str(i).split()\n",
        "    xx = \"\"\n",
        "    for j in arr:\n",
        "        j = str(j).lower()\n",
        "        if j[:4] == 'http' or j[:3] == 'www':\n",
        "            continue\n",
        "        if j in keys:\n",
        "            # print(\"inn\")\n",
        "            j = appo[j]\n",
        "        xx += j + \" \"\n",
        "    new_train_data.append(xx)\n",
        "\n",
        "train[\"comment_text\"] = new_train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dEqRHxqk0AVx"
      },
      "source": [
        "new_test_data = []\n",
        "lte = test[\"comment_text\"].tolist()\n",
        "\n",
        "for i in lte:\n",
        "    arr = str(i).split()\n",
        "    xx = \"\"\n",
        "    for j in arr:\n",
        "        j = str(j).lower()\n",
        "        if j[:4] == 'http' or j[:3] == 'www':\n",
        "            continue\n",
        "        if j in keys:\n",
        "            # print(\"inn\")\n",
        "            j = appo[j]\n",
        "        xx += j + \" \"\n",
        "    new_test_data.append(xx)\n",
        "\n",
        "test[\"comment_text\"] = new_test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "peyHP6JH0AV0"
      },
      "source": [
        "trate = train[\"comment_text\"].tolist()\n",
        "tete = test[\"comment_text\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0eErVR1O0AV3"
      },
      "source": [
        "for i, c in enumerate(trate):\n",
        "    trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n",
        "    trate[i] = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",trate[i]) #remove ip\n",
        "    trate[i] = re.sub(\"\\[\\[.*\\]\",\"\",trate[i]) #remove user_name\n",
        "for i, c in enumerate(tete):\n",
        "    tete[i] = re.sub('[^a-zA-Z ?!]+', '', tete[i])\n",
        "    tete[i] = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",tete[i])\n",
        "    tete[i] = re.sub(\"\\[\\[.*\\]\",\"\",tete[i])\n",
        "train[\"comment_text\"] = trate\n",
        "test[\"comment_text\"] = tete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hjeOBwiP0AV7"
      },
      "source": [
        "X_train = list(train[\"comment_text\"].str.lower())\n",
        "y_train = train[labels].values\n",
        "X_test = list(test[\"comment_text\"].str.lower())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycm_XJ3G3WpB"
      },
      "source": [
        "**Evaluation Metric**\n",
        "\n",
        "To evaluate the model, I'll be looking at its AUC ROC score (area under the receiver operating characteristic curve). I will be looking at the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative one. With data that mostly consists of negative labels (no toxicity), our model could just learn to always predict negative and end up with a pretty high accuracy. AUC ROC helps correct this by putting more weight on the the positive examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q1WRZaRc0AV9"
      },
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Tf9cJV230AWA"
      },
      "source": [
        "def plot_graphs(history, metric, optimizer):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    plt.title(optimizer)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdGYF9bG3a-d"
      },
      "source": [
        "**Tokenizing**\n",
        "\n",
        "When dealing with text, it is typical to assign a unique integer id to each word in the corpus. This makes it simpler to represent the data as we feed it into a model.\n",
        "\n",
        "Because this is such a common operation, Keras has built-in support for it. We need to fit the tokenizer object on our corpus so that it can assign a unique id to each word. We also initialize the tokenizer with a num_words parameter so that it knows how many of the top frequency words to take into account.\n",
        "\n",
        "**Padding**\n",
        "\n",
        "Our model would expect each input row to be of the same dimension. This means once we decide on what the chosen input comment size is, we need to chop off some part of the input comments that are too long. We would also, need to pad the comments which are shorter.\n",
        "\n",
        "Keras has built in support for this using the pad_sequences method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Iq-dQgaL0AWC"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS,lower=True) # define Tokenize text function\n",
        "tokenizer.fit_on_texts(X_train) #fit the function on the text\n",
        "X_train = tokenizer.texts_to_sequences(X_train) # convert  to sequence\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "word_index = tokenizer.word_index #num of unique tokens\n",
        "print('Vocabulary size:', len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HDY7Qh5R0AWF"
      },
      "source": [
        "#Limit size  to 200 and pad the sequence\n",
        "data = pad_sequences(X_train, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n",
        "data_test = pad_sequences(X_test, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj2Cuaun0AWI"
      },
      "source": [
        "### Word Embedding\n",
        "\n",
        "Use pretrained GloVe model from Stanford  https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A8gr_qdn0AWI"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(GLOVE_DIR)\n",
        "print('Loading GloVe from:', GLOVE_DIR,'...', end='')\n",
        "\n",
        "for line in f:\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()\n",
        "print(\"Done.\\n Proceeding with Embedding Matrix...\")\n",
        "print(f'Found {len(embeddings_index)} word vectors', end=\"\")\n",
        "\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(\" Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "f22HRvAG0AWL"
      },
      "source": [
        "# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "# embedding_layer = Embedding(len(word_index) + 1,\n",
        "#                            EMBEDDING_DIM,\n",
        "#                            weights = [embedding_matrix],\n",
        "#                            input_length = MAX_SEQUENCE_LENGTH,\n",
        "#                            trainable=False,\n",
        "#                            name = 'embeddings')\n",
        "# embedded_sequences = embedding_layer(sequence_input)\n",
        "# x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
        "# x = GlobalMaxPool1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(50, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# preds = Dense(6, activation=\"sigmoid\")(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Lp8bwSdW0AWN"
      },
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                           EMBEDDING_DIM,\n",
        "                           weights = [embedding_matrix],\n",
        "                           input_length = MAX_SEQUENCE_LENGTH,\n",
        "                           trainable=False,\n",
        "                           name = 'embeddings')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = SpatialDropout1D(0.2)(embedded_sequences)\n",
        "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
        "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "x = concatenate([avg_pool, max_pool])\n",
        "preds = Dense(6, activation=\"sigmoid\")(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v7vT_TOq0AWQ"
      },
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "36vh_LST0AWS"
      },
      "source": [
        "model = Model(sequence_input, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KiuMIR580AWU"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 4\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(data, y_train, train_size=0.8, random_state=233)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQuHSW680AWX"
      },
      "source": [
        "Optimizer: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Cf9GsVJC0AWY"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6oMwDrOw0AWa"
      },
      "source": [
        "ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
        "callbacks_list = [ra_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1YTrX_o90AWd"
      },
      "source": [
        "hist_adam = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
        "print('Predicting....')\n",
        "y_pred_Adam = model.predict(data_test,batch_size=1024,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YKkXK39_0AWf"
      },
      "source": [
        "plot_graphs(hist_adam, 'accuracy', 'Adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "twJ7rQ-20AWh"
      },
      "source": [
        "plot_graphs(hist_adam, 'loss', 'Adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkroXNIo0AWk"
      },
      "source": [
        "Optimizer SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IH7kBLzq0AWl"
      },
      "source": [
        "opt = SGD(lr=0.01, momentum=0.9, decay=0.0001)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "hist_sgd = model.fit(X_tra, y_tra, batch_size=400, epochs=20, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
        "print('Predicting....')\n",
        "y_pred_SGD = model.predict(data_test,batch_size=1024,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RWGT4uG10AWp"
      },
      "source": [
        "plot_graphs(hist_sgd, 'accuracy', 'Adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "z-_6ntKm0AWr"
      },
      "source": [
        "plot_graphs(hist_sgd, 'loss', 'Adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zODJEa80AWt"
      },
      "source": [
        "Optimizer Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n9VV3rFe0AWu"
      },
      "source": [
        "opt = Adadelta()\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "hist_ada = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
        "print('Predicting....')\n",
        "y_pred_Ada = model.predict(data_test,batch_size=1024,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2uRq-sHx0AWw"
      },
      "source": [
        "plt.suptitle('Optimizer : Adadelta', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.plot(hist_ada.history['acc'], color='b', label='Training Accuracy')\n",
        "plt.plot(hist_ada.history['val_acc'], color='r', label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Bj-jp8Nu0AWy"
      },
      "source": [
        "# submission = pd.read_csv('../input/toxic-comment/submission_gru.csv')\n",
        "# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
        "# submission.to_csv('preprocess_gru_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}